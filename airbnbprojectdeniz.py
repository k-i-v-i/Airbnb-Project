# -*- coding: utf-8 -*-
"""AirbnbProjectDeniz

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KFewWuVwjkEJY84jAMf8WDgUxROsdtxD

# Project 2: New York City Airbnb Prices
 [ Group 66 ]

Group Members:

* Deniz Incereis
* Onur Bacaksız
* Bilgehan Çağıltay

## Introduction
Our projects main dataset describes the listing activity and metrics in New York City, NY for 2019. It involves host informations including reviews and listings count, and also involves the characteristics of the house such as neighbourhood, room type and availability.

---



> Our aim is to analyze the statistics and distributions, and to detect some relations between the features of host/house, and the prices. We will also share the statistical similarities and differences of different neighbourhood groups, also by taking into account our additional datasets.

# **MACHINE LEARNING**

In this section, we used some of the regression models to predict our target variable 'price'.
Also introduced regularization and hyper-parameter tuning for some of the models.

* First, let's start with preprocessing our dataset to be able to fit our data into models.
"""

filename_main = "AB_NYC_2019.csv"
main_df = pd.read_csv(join(path_prefix, filename_main))
main_df.head()

# Since our main dataset includes the latitude and longitude in degrees, we converted from radians to degrees
# and add as a new feature
import math

file_poi = "poi.csv"
dataf = pd.read_csv(join(path_prefix, file_poi))
filter = dataf["name"].str.contains("NEW YORK") # filtered the data
poi_df = dataf[filter]

poi_df["latitude"] = poi_df["latitude_radian"].apply(math.degrees)
poi_df["longitude"] = poi_df["longitude_radian"].apply(math.degrees)
poi_df.tail(3)

# Function to find the neighbourhood group of a given POI,
# with the calculated min and max values of latitude and longitude from the main dataset
def find_neighbourhood_group(latitude, longitude):
  if (latitude >= 40.56546 and latitude <= 40.79721) and (longitude >= -73.71299 and longitude <= -73.95927):
    return "Queens"
  elif (latitude >= 40.70234 and latitude <= 40.87665) and (longitude >= -74.01851 and longitude <= -73.90855):
    return "Manhattan"
  elif (latitude >= 40.57115 and latitude <= 40.7389) and (longitude >= -74.03942 and longitude <= -73.85676):
    return "Brooklyn"
  elif (latitude >= 40.80011 and latitude <= 40.91306) and (longitude >= -73.9319 and longitude <= -73.78158):
    return "Bronx"
  elif (latitude >= 40.49979 and latitude <= 40.64779) and (longitude >= -74.24442 and longitude <= -74.06092):
    return "Staten Island"
  else:
    return "unknown location"

# We add the neighbourhood group as a column
poi_df["neighbourhood_group"] = poi_df.apply(lambda x: find_neighbourhood_group(x["latitude"],x["longitude"]), axis = 1)
poi_df.head(3)

poi_df = pd.DataFrame(poi_df, columns = ["name", "neighbourhood_group", "latitude", "longitude"])
poi_df = poi_df[poi_df["neighbourhood_group"] != "unknown location"]
poi_df.head(10)

main_poi_df = pd.DataFrame(main_df, columns = ["name", "neighbourhood_group", "latitude", "longitude", "price"])
main_poi_df.head(10)

main_poi_df["POI"] = 0
main_poi_df.tail(10)

def find_poi_count(latitude, longitude) :
  count = 0
  R = 6373.0
  lat1 = math.radians(latitude)
  lon1= math.radians(longitude)

  for i in poi_df.index:
    lat2 = math.radians(poi_df["latitude"][i])
    lon2 = math.radians(poi_df["longitude"][i])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    distance = R * c
    if (distance < 0.8) :
      count = count + 1
  return count

main_poi_df["POI"] = main_poi_df.apply(lambda x: find_poi_count(x["latitude"],x["longitude"]), axis = 1)
main_poi_df["Close_crimes"] = 0
main_poi_df.head(10)

"""Now, we find the POI counts for every Airbnb house.

Next step is to find the crime counts.

"""

nypd_filename = "NYPD_Complaint_Data_Current__Year_To_Date_.csv"
nypd_df = pd.read_csv(join(path_prefix, nypd_filename))

#more optimized method. Sorts the list and counts the closest ones
import math
from sklearn.neighbors import BallTree
from math import radians

def find_crime_bnb_count_optimized(lat, lon) :
  count = 0
  R = 6373.0

  lat1 = math.radians(lat)
  lon1= math.radians(lon)

  for i in range(int(crime_locs[nearest_crime_indices[0]].size/2)):
    lat2 = math.radians(nypd_df["Latitude"][(nearest_crime_indices[0])[i]])
    lon2 = math.radians(nypd_df["Longitude"][(nearest_crime_indices[0])[i]])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    distance = R * c
    if (distance < 1.0) :
      count = count + 1
    else: #as soon as distance exceeds the set amount, return. Saves time
      return count
  return count


house_locs = main_df[["latitude", "longitude"]].values
crime_locs = nypd_df[["Latitude", "Longitude"]].values

# radians için bir fonksiyon
r = np.vectorize(lambda x: radians(x))
# lat-lng değerlerini radians'a çevir

house_locs = r(house_locs)
crime_locs = r(crime_locs)

# ballTree'ye crime listesini gir
# haversine, yeryüzü üzerindeki noktalar arasındaki mesafeler için
tree = BallTree(crime_locs, metric="haversine")

for i in range(len(house_locs)-1):
  # oluşan tree'den listing loc'ları için en yakın noktaları çek
  nearest_distances, nearest_crime_indices = tree.query(house_locs[i:i+2], k=int(len(crime_locs)/100))
  #/2 because the last half of closest crimes are probably not in range. Optimizes the solution a bit

  main_poi_df["Close_crimes"][i] = find_crime_bnb_count_optimized(main_df["latitude"][i], main_df["longitude"][i])

main_poi_df.head()

"""Now we have the close crimes and POI counts for every Airbnb house.

Let's also find the distance between the Airbnb house and the closest bus stop.
"""

file_busstop = "Bus_Stop_Shelter.csv"
busstop_df = pd.read_csv(join(path_prefix, file_busstop))

main_poi_df["Bus Stop"] = 0.0

from sklearn.neighbors import BallTree
from math import radians

house_locs = main_poi_df[["latitude", "longitude"]].values
busstop_locs = busstop_df[["LATITUDE", "LONGITUDE"]].values

# a function for radians
r = np.vectorize(lambda x: radians(x))
# lat-lng change values to radian

house_locs = r(house_locs)
busstop_locs = r(busstop_locs)

# haversine, is points for distances
tree = BallTree(busstop_locs, metric="haversine")
nearest_distances, nearest_busstop_indices = tree.query(house_locs)

for i in range(len(house_locs)-1) :
  main_poi_df["Bus Stop"][i] = nearest_distances[i]

print(nearest_distances[0])

main_poi_df.head()
# We calculated the distance between the busstop and airBnB houses

"""Since we utilized all the information from additional datasets, now we can normalize/standardize the features necessary features."""

#Normalization for the distance of the nearest bus stop
main_poi_df["Bus Stop"] = (main_poi_df["Bus Stop"] - main_poi_df["Bus Stop"].min()) / (main_poi_df["Bus Stop"].max() - main_poi_df["Bus Stop"].min())

#Normalization for close crimes count
main_poi_df["Close_crimes"] = (main_poi_df["Close_crimes"] - main_poi_df["Close_crimes"].min()) / (main_poi_df["Close_crimes"].max() - main_poi_df["Close_crimes"].min())
main_poi_df.head()

#Since we will concatenate the dataframes, we get rid of the duplicate columns
main_poi_df = pd.DataFrame(main_poi_df, columns= ["POI","Close_crimes","Bus Stop"])

"""We will encode the categorical features with One Hot Encoding method, and concatenate with the main dataframe."""

roomtype_coded = pd.get_dummies(main_df["room_type"])
neighbourhood_group_coded = pd.get_dummies(main_df["neighbourhood_group"])
neighbourhood_coded = pd.get_dummies(main_df["neighbourhood"])

main_df = pd.concat([main_df,main_poi_df, roomtype_coded, neighbourhood_group_coded, neighbourhood_coded], axis=1, join='inner')
main_df.head()

"""Now we will get rid of the uninformative columns and remove NaN rows."""

main_df.dropna(inplace=True)
new_main = main_df.drop(["name", "id", "host_id", "last_review", "host_name","neighbourhood_group","neighbourhood", "room_type", "latitude", "longitude"], axis=1)
new_main.head()

"""When we check the data types, we still have floating numbers. We will convert them to integer type."""

new_main.dtypes

new_main['reviews_per_month'] = pd.to_numeric(new_main['reviews_per_month'], errors = 'coerce')
new_main['reviews_per_month'] = new_main['reviews_per_month'].astype('int64')

new_main["Close_crimes"] = new_main["Close_crimes"] * 100000
new_main["Bus Stop"] = new_main["Bus Stop"] * 100000
new_main.head()

new_main["Close_crimes"] = new_main["Close_crimes"].astype("int64")
new_main['Bus Stop'] = new_main['Bus Stop'].astype('int64')
new_main.head()

"""Now, we will split our data into test and train."""

from sklearn.model_selection import train_test_split
numerical_features =  new_main.select_dtypes(exclude=['object'])
y = numerical_features.price
numerical_features = numerical_features.drop(['price'], axis=1)
X = numerical_features

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print('Dimensions of the training feature matrix: {}'.format(X_train.shape))
print('Dimensions of the training target vector: {}'.format(y_train.shape))
print('Dimensions of the test feature matrix: {}'.format(X_test.shape))
print('Dimensions of the test target vector: {}'.format(y_test.shape))

#Removed the duplicate columns, if any.
X_train = X_train.loc[:,~X_train.columns.duplicated()]
X_test = X_test.loc[:,~X_test.columns.duplicated()]

from sklearn.preprocessing import scale, StandardScaler, RobustScaler
#Scaled the data with robust scaler.
scaler = RobustScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.fit_transform(X_test)

"""##Liner Regression Models

In this section, we used some common linear regression models, and compared the r squared scores to find the best model.

Also introduced regularization with the parameter alpha for Lasso, Ridge and Elastic Net models.

###OLS (Ordinary Least Squares)
"""

from sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge, ElasticNet
from sklearn.metrics import explained_variance_score
ols = LinearRegression()
ols.fit(X_train, y_train)
ols_yhat = ols.predict(X_test)
ols_ytrain = ols.predict(X_train)

#print(ols.score(X_train, y_train))  #score returns r_square
print(ols.score(X_test, y_test))

#print(explained_variance_score(y_test, ols_yhat))
print(mean_absolute_error(ols_ytrain, y_train))

"""###Ridge

Here, the parameter alpha corresponds to the regularization coefficient. To avoid overfitting we arranged it to 0.5, also 0.5 gives the best r squared score.
"""

ridge = Ridge(alpha = 0.5, normalize=True) #TRY WITH AND WITHOUT NORMALIZING, THE SCORE IMPROVES
ridge.fit(X_train, y_train)
ridge_ytrain = ridge.predict(X_train)
ridge_yhat = ridge.predict(X_test)
#explained_variance_score(y_test, ridge_yhat)

print(mean_absolute_error(ridge_ytrain, y_train))
ridge.score(X_test, y_test)

"""###Lasso

Just like Ridge model, we have the hyperparameters alpha and normalize, by increasing the alpha value (default is 0.01 not sure though????) we get a more generalized result and avoid overfitting.
"""

lasso = Lasso(alpha = 0.03, normalize=True) # AGAIN CHANGE THE ALPHA VALUE
lasso.fit(X_train, y_train)
lasso_ytrain = lasso.predict(X_train)
lasso_yhat = lasso.predict(X_test)
#explained_variance_score(y_test, lasso_yhat)

print(mean_absolute_error(lasso_ytrain, y_train))
lasso.score(X_test, y_test)

"""###Bayesian


"""

bayesian = BayesianRidge()
bayesian.fit(X_train, y_train)
bayesian_ytrain = bayesian.predict(X_train)
bayesian_yhat = bayesian.predict(X_test)
#explained_variance_score(y_test, bayesian_yhat)

print(mean_absolute_error(bayesian_ytrain, y_train))
bayesian.score(X_test, y_test)

"""###Elastic Net

In Elastic Net model, we have 3 hyperparameters. This model combines L1 and L2 regularization we performed in Lasso and Ridge.

l1_ratio corresponds to Lasso penalty and alpha corresponds to the constant that multiplies the coefficients.
"""

en = ElasticNet(alpha = 0.01, l1_ratio=0.5, normalize=False)
en.fit(X_train, y_train)
en_ytrain = en.predict(X_train)
en_yhat = en.predict(X_test)

print(mean_absolute_error(en_ytrain, y_train))
en.score(X_test, y_test)

"""* Comparing the scores of the linear regression models, the best model that fits our data is the Ridge model where we performed L2 regularization with hyperparameter alpha.

##XGB Regressor

One of the regression models we use is XGBoost Regressor.

Here we performed a KFold cross validation with 5 folds.
"""

# evaluate an xgboost regression model on the housing dataset
from numpy import absolute
from pandas import read_csv
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold, KFold
from xgboost import XGBRegressor
# load the dataset
# define model
xgb_baseline  = XGBRegressor(n_estimators=1000, learning_rate=0.05, early_stopping=5, max_depth=5, min_child_weight=1, gamma = 0.2)
# define model evaluation method
kf = KFold(5, shuffle=True, random_state = 91).get_n_splits(numerical_features)

# evaluate model
scores = cross_val_score(xgb_baseline , X_train, y_train, scoring='neg_mean_absolute_error', cv=kf)
# force scores to be positive
scores = absolute(scores)
print('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )

#HERE WE DO NOT USE ANY HYPERPARAMETERS

from sklearn.metrics import mean_absolute_error
xgb = XGBRegressor()
xgb.fit(X_train, y_train)
xgb_yhat = xgb.predict(X_test)

mean_absolute_error(xgb_yhat, y_test)

#WITH THE HYPERPARAMETERS, THE RESULT IMPROVED

xgb_baseline = XGBRegressor(n_estimators=1000, learning_rate=0.05, early_stopping=5, max_depth=5, min_child_weight=1, gamma = 0.2)
xgb_baseline.fit(X_train, y_train)
y_train_xgb_base = xgb_baseline.predict(X_train)
y_test_xgb_base = xgb_baseline.predict(X_test)
mean_absolute_error(y_train_xgb_base, y_train)

"""* As it can be seen from the mean absolute error values of the corresponding model with and without hyperparameters, with the correct hyperparameter tuning we managed to reduce the error.

##MLP (Multi-layer Perceptron)

Other than XGBoost, as a neural network model we used MLP.
"""

from sklearn.neural_network import MLPRegressor
from sklearn.datasets import make_regression

regr = MLPRegressor(random_state=1, max_iter=500, hidden_layer_sizes=9)
regr.fit(X_train, y_train)
regr_ytrain = regr.predict(X_train)
regr.predict(X_test)

print(mean_absolute_error(regr_ytrain, y_train))
regr.score(X_test, y_test)

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""Here, to find the best hyperparameter setting, we used the grid search cross validation algorithm and input the parameters we want to try.

* This returns us the best hyperparameter setting for MLP model
"""

#NOW LET'S FIND THE BEST HYPERPARAMETERS
tuned_parameters = {'activation':("identity", "logistic", "tanh", "relu"), 'alpha': [0.001, 0.01, 0.05],  'hidden_layer_sizes': [1,2,3,4,5,6,7,8,9,10], 'learning_rate':("constant", "invscaling","adaptive"), 'learning_rate_init': [0.001, 0.01, 0.1],  'solver':("lbfgs", "sgd", "adam")}

from sklearn.model_selection import GridSearchCV
rgr = GridSearchCV(MLPRegressor(), tuned_parameters, cv=5)
rgr.fit(X_train, y_train)
sorted(rgr.cv_results_.keys())

"""##Random Forest

Another neural network based model we used is Random Forest.

Just like we did in MLP, we used grid search to find the best hyperparameter seting.
"""

from sklearn.ensemble import RandomForestRegressor

rfor = RandomForestRegressor(max_depth=2, random_state=0)
rfor.fit(X_train, y_train)
rfor_ytrain = rfor.predict(X_train)
rfor.predict(X_test)

print(mean_absolute_error(rfor_ytrain, y_train))
rfor.score(X_test, y_test)

#AGAIN, TO FIND THE BEST HYPERPARAMETER SETTINGS, USED GRID SEARCH
tuned_parameters = {'max_depth': [2, 5, 8],'n_estimators': [50, 80, 100], 'min_samples_split' : [0.5,1,2,3], 'max_features': ("auto", "sqrt", "log2"), 'random_state': [0,3,5]}

rfor_rgr = GridSearchCV(RandomForestRegressor(), tuned_parameters, cv=5)
rfor_rgr.fit(X_train, y_train)
print(sorted(rfor_rgr.cv_results_.keys()))
rfor_rgr.score(X_test, y_test)

"""# Utilized Datasets

* We are using the POI data that involes 400,000 unique points of interest in the whole world. We did the necessary filtering and manipulation with explanations in the "Point of Interest Section"
[Points of Interest POI Database](https://www.kaggle.com/ehallmar/points-of-interest-poi-database)

*We are using the Bus Stop data that involes 3,400 unique bus stop in the New York city.We did the necessary manipulation with explanations in the "Bus Stop Shelters".
[Bus Stop Shelters Database](https://data.cityofnewyork.us/Transportation/Bus-Stop-Shelters/qafz-7myz?category=Transportation&view_name=Bus-Stop-Shelters)


* We are also using the NYPD Complaint Data, which involves 413,412 complaints reported in New York City. We performed a scoring system which will be discussed in detail in NYPD Crime Statistics section
[NYPD Complaint Data Current](https://data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Current-Year-To-Date-/5uac-w243/data)


---


"""

from google.colab import drive
drive.mount("./drive", force_remount=True)

path_prefix = "./drive/My Drive"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from os.path import join
from scipy.stats import f_oneway
from sklearn.preprocessing import StandardScaler

# %matplotlib inline

"""## Point of Interest Statistics#

In  this section, we analyzed our the POI's statistics and how they vary according to the neighbourhood groups, including their distributions and visual explanations.
"""

# This part shows the POI's located in New York State
file_poi = "poi.csv"
dataf = pd.read_csv(join(path_prefix, file_poi))
filter = dataf["name"].str.contains("NEW YORK") # filtered the data
poi_df = dataf[filter]
poi_df.head()

poi_df.describe() #descriptive statistics of POI file

poi_df.dtypes

poi_df.info()

# Since our main dataset includes the latitude and longitude in degrees, we converted from radians to degrees
# and add as a new feature
import math
poi_df["latitude"] = poi_df["latitude_radian"].apply(math.degrees)
poi_df["longitude"] = poi_df["longitude_radian"].apply(math.degrees)
poi_df.head()

# Function to find the neighbourhood group of a given POI,
# with the calculated min and max values of latitude and longitude from the main dataset
def find_neighbourhood_group(latitude, longitude):
  if (latitude >= 40.56546 and latitude <= 40.79721) and (longitude >= -73.71299 and longitude <= -73.95927):
    return "Queens"
  elif (latitude >= 40.70234 and latitude <= 40.87665) and (longitude >= -74.01851 and longitude <= -73.90855):
    return "Manhattan"
  elif (latitude >= 40.57115 and latitude <= 40.7389) and (longitude >= -74.03942 and longitude <= -73.85676):
    return "Brooklyn"
  elif (latitude >= 40.80011 and latitude <= 40.91306) and (longitude >= -73.9319 and longitude <= -73.78158):
    return "Bronx"
  elif (latitude >= 40.49979 and latitude <= 40.64779) and (longitude >= -74.24442 and longitude <= -74.06092):
    return "Staten Island"
  else:
    return "unknown location"

# We add the neighbourhood group as a column
poi_df["neighbourhood_group"] = poi_df.apply(lambda x: find_neighbourhood_group(x["latitude"],x["longitude"]), axis = 1)
poi_df.head()

neigh_group = poi_df.groupby(by="neighbourhood_group")
neigh_group["neighbourhood_group"].value_counts()

# Since after the filtering we have the whole New York State, we just have 72 POI's in New York City,
# which we are interested in

# Now we just have the POI's located in New York City
poi_df = poi_df[poi_df["neighbourhood_group"] != "unknown location"]
poi_df.head()

# modified the dataframe by converting it into another dataframe that does not include the unnecessary info
poi_df = pd.DataFrame(poi_df, columns = ["name", "neighbourhood_group", "latitude", "longitude"])
poi_df.head()

poi_df.describe() # descriptive statistics of the new dataframe

"""###Visualizations of the features


---



---


"""

# visualized the locations of the POI's which are in the New York City,
# by pinning the locations on an open street map
!pip install plotly_express
import plotly_express as px
fig = px.scatter_mapbox(poi_df, lat="latitude", lon="longitude", hover_name="name", hover_data=["neighbourhood_group"],
                        color_discrete_sequence=["red"], zoom=10, height=300)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

# Also visualized the POI's according their number of occurences with a heat map
import folium
!pip install folium.plugins
from folium.plugins import HeatMap

map = folium.Map(location=[40.786764, -73.935811], zoom_start=10, width=1000, height=500, tiles = "Stamen Terrain")

locations = poi_df[["latitude", "longitude"]].dropna(how="any").values

HeatMap(locations).add_to(map)

map

# This part shows the numerical distribution of the POI's over the neighbourhood groups in NYC
neigh_group = poi_df.groupby(by="neighbourhood_group")
neigh_group["neighbourhood_group"].value_counts()
# It can be seen that most of the POI's are located in Manhattan and Bronx,
# where in Staten Island and Brooklyn has few -> 0.15% of the total POI's located in S.I. and Brooklyn

# This graph shows the distribution of the POI's with respect to their neighbourhood group
ax = poi_df["neighbourhood_group"].value_counts(ascending=True)[-5:].plot(kind="barh", color="steelblue")
ax.set_xlabel('Occurrences')
ax.set_title("Neighbourhood Groups and POI's ")
ax.grid(axis='x', color='white');

"""###Statistical Similarities and Differences
It can be seen that the most part of the Points of Interest are located in Manhattan and Bronx; where on the other hand we get the least number of POI's in Brooklyn and Staten Island. As we can see clearly from the heat map, especially around Central Park there is a intense number of POI's, which can help us to make a prediction, since there is clearly a difference.
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from os.path import join
from scipy.stats import f_oneway

# %matplotlib inline

filename_main = "AB_NYC_2019.csv"
main_df = pd.read_csv(join(path_prefix, filename_main))
main_df.head()

# This part shows the POI's located in New York State
file_poi = "poi.csv"
dataf = pd.read_csv(join(path_prefix, file_poi))
filter = dataf["name"].str.contains("NEW YORK") # filtered the data
poi_df = dataf[filter]
poi_df.head()

# Since our main dataset includes the latitude and longitude in degrees, we converted from radians to degrees
# and add as a new feature
import math
poi_df["latitude"] = poi_df["latitude_radian"].apply(math.degrees)
poi_df["longitude"] = poi_df["longitude_radian"].apply(math.degrees)
poi_df.tail(3)

# Function to find the neighbourhood group of a given POI,
# with the calculated min and max values of latitude and longitude from the main dataset
def find_neighbourhood_group(latitude, longitude):
  if (latitude >= 40.56546 and latitude <= 40.79721) and (longitude >= -73.71299 and longitude <= -73.95927):
    return "Queens"
  elif (latitude >= 40.70234 and latitude <= 40.87665) and (longitude >= -74.01851 and longitude <= -73.90855):
    return "Manhattan"
  elif (latitude >= 40.57115 and latitude <= 40.7389) and (longitude >= -74.03942 and longitude <= -73.85676):
    return "Brooklyn"
  elif (latitude >= 40.80011 and latitude <= 40.91306) and (longitude >= -73.9319 and longitude <= -73.78158):
    return "Bronx"
  elif (latitude >= 40.49979 and latitude <= 40.64779) and (longitude >= -74.24442 and longitude <= -74.06092):
    return "Staten Island"
  else:
    return "unknown location"

# We add the neighbourhood group as a column
poi_df["neighbourhood_group"] = poi_df.apply(lambda x: find_neighbourhood_group(x["latitude"],x["longitude"]), axis = 1)
poi_df.head(3)

neigh_group = poi_df.groupby(by="neighbourhood_group")
neigh_group["neighbourhood_group"].value_counts()

poi_df = poi_df[poi_df["neighbourhood_group"] != "unknown location"]
poi_df.head()

poi_df = pd.DataFrame(poi_df, columns = ["name", "neighbourhood_group", "latitude", "longitude"])
poi_df.head(10)

poi_df.shape

main_poi_df = pd.DataFrame(main_df, columns = ["name", "neighbourhood_group", "latitude", "longitude", "price"])
main_poi_df.head(10)

main_poi_df["POI"] = 0
main_poi_df.tail(10)

poi_df.head()

"""##Statistical Tests to Check How POI Counts Affect Airbnb Prices

In this section we want to test if there are significant differences in terms of proximity to Points of Interest. To do this analyze, we determined the proximity distance as 800 meters, and counted the number of POI occurences within this distance.

**Null Hypothesis ($H_0$)**: Means of `prices` of the sample from the houses that has at least 1 POI are the same as the sample from houses that do not have any POI in 0.8 kilometers. (e.g. $ci_1$ denotes the sample price of the houses that have at least 1 POI)

$ H_0: \mu_{ci_1} = \mu_{ci_2} $

**Alternative Hypothesis ($H_A$)**: Means of `prices` samples of the houses that has at least 1 POI are different than the sample from houses that do not have any POI.

$ H_A:$ Means $\mu_{ci_1}, \mu_{ci_2}$ are not same.

**Significance level**: As most of hypothesis tests assume significance level as `0.05`, we are setting it as `0.05` for our test too.
"""

def find_poi_count(latitude, longitude) :
  count = 0
  R = 6373.0
  lat1 = math.radians(latitude)
  lon1= math.radians(longitude)

  for i in poi_df.index:
    lat2 = math.radians(poi_df["latitude"][i])
    lon2 = math.radians(poi_df["longitude"][i])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    distance = R * c
    if (distance < 0.8) :
      count = count + 1
  return count

main_poi_df["POI"] = main_poi_df.apply(lambda x: find_poi_count(x["latitude"],x["longitude"]), axis = 1)
main_poi_df.head(10)

high_poi_df = main_poi_df.loc[main_poi_df["POI"] >= 1] #High sample
high_poi_df.head()

low_poi_df = main_poi_df.loc[main_poi_df["POI"] < 1] #Low sample
low_poi_df.head()

from scipy import stats
from scipy import special
from os.path import join
import seaborn as sns
# mean and std values from the sample
n =  main_poi_df["price"].count()
mean = main_poi_df["price"].mean()
std = main_poi_df["price"].std() / np.sqrt(n)
offset = 4*std

# the x-axis ticks of the plot
# generates 100 equally separated ticks
x = np.linspace(mean - offset, mean + offset, n)

# probability density function
# of the given normal dist.
y = stats.norm.pdf(x, mean, std)

plt.figure(figsize=(10, 5))
plt.plot(x,y)
# put grids on the figure
plt.grid()
plt.xlabel("x")
plt.ylabel("Normal Distribution")
plt.fill_between(x, y, alpha=0.3, color="b")
plt.title(f"PRICE DISTRIBUTION")
plt.show()

fig, ax = plt.subplots(1, 4, figsize=(14,5))

high_poi_df['price'].plot(kind="hist", ax=ax[0], bins=100, label="more than 1 poi", color="c", density=True)
ax[0].set_title("Target: 0")
ax[0].set_xscale("log")

main_poi_df['price'].plot(kind="hist", ax=ax[1], bins=100, label="random sample", color="m", density=True)
ax[1].set_title("Target: 1")
ax[1].set_xscale("log")

low_poi_df['price'].plot(kind="hist", ax=ax[2], bins=100, label="less than 1 poi", color="y", density=True)
ax[2].set_title("Target: 2")
ax[2].set_xscale("log")

sns.kdeplot(high_poi_df['price'], shade=True, label="Target: 0", ax=ax[3], color="c")
sns.kdeplot(main_poi_df['price'], shade=True, label="Target: 1", ax=ax[3], color="m")
sns.kdeplot(low_poi_df['price'], shade=True, label="Target: 2", ax=ax[3], color="y")
ax[3].set_title("Comparison with KDE")
ax[3].set_xscale("log")

plt.suptitle("Color Intensity Distributions")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# mean and std values from the sample
n =  main_poi_df["price"].count()
mean = main_poi_df["price"].mean()
std = main_poi_df["price"].std() / np.sqrt(n)

# sample mean
sample_mean = high_poi_df["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

# mean and std values from the sample
n =  main_poi_df["price"].count()
mean = main_poi_df["price"].mean()
std = main_poi_df["price"].std() / np.sqrt(n)

# sample mean
sample_mean = low_poi_df["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

"""* From these results, it can be seen that the prices of the houses that have at least 1 poi in 0.5km are way more higher than all houses in the dataset.
* Also the prices of the houses that have no poi in 0.5km are lower than all houses in the dataset.

Now let's check if these differences are significant or not.
"""

stats.ttest_ind(high_poi_df["price"], low_poi_df["price"], equal_var=False)

"""* We can reject the null hypothesis since our p value is less than 0.05, suggesting that there is a significant difference between the mean price of the Airbnb houses that has at least 1 POI and the mean price of the Airbnb houses that do not have any POI located within 0.8 km.
* Also by looking at the z-score, we can state that the mean price of houses that has at least 1 POI is greater than the population average; the mean price of the houses that do not have any POI is less than the population average. -> And clearly we can state that having at least 1 POI increases the price.

##Statistical Tests to Check How POI Counts Affect Airbnb Prices in Different Neighborhoods
"""

man_poi = main_poi_df[main_poi_df["neighbourhood_group"] == "Manhattan"]
bro_poi = main_poi_df[main_poi_df["neighbourhood_group"] == "Brooklyn"]
stat_poi = main_poi_df[main_poi_df["neighbourhood_group"] == "Staten Island"]
bronx_poi = main_poi_df[main_poi_df["neighbourhood_group"] == "Bronx"]

"""###Manhattan

* Start with detecting outliers and removing them, to get a valid result.
"""

Q1 = man_poi["price"].quantile(0.25)
Q3 = man_poi["price"].quantile(0.75)
IQR = Q3 - Q1
print(IQR)

man_poi.shape

new_man = man_poi[~((man_poi["price"] < (Q1 - 1.5 * IQR)) |(man_poi["price"] > (Q3 + 1.5 * IQR)))]
new_man.shape

man_high = new_man[new_man["POI"] >= 1] #HIGH POI SAMPLE
man_low = new_man[new_man["POI"] < 1] #LOW POI SAMPLE

"""* Now, since we divided the dataset into 2 groups, let's visualize the differences of these 2 groups' means and the population mean."""

man_poi.describe()

fig, ax = plt.subplots(1, 4, figsize=(14,5))

man_high['price'].plot(kind="hist", ax=ax[0], bins=10, label="more than 1 poi", color="c", density=True)
ax[0].set_title("Target: 0")
ax[0].set_xscale("log")

man_poi['price'].plot(kind="hist", ax=ax[1], bins=10, label="random sample", color="m", density=True)
ax[1].set_title("Target: 1")
ax[1].set_xscale("log")

man_low['price'].plot(kind="hist", ax=ax[2], bins=10, label="less than 1 poi", color="y", density=True)
ax[2].set_title("Target: 2")
ax[2].set_xscale("log")

sns.kdeplot(man_high['price'], shade=True, label="Target: 0", ax=ax[3], color="c")
sns.kdeplot(man_poi['price'], shade=True, label="Target: 1", ax=ax[3], color="m")
sns.kdeplot(man_low['price'], shade=True, label="Target: 2", ax=ax[3], color="y")
ax[3].set_title("Comparison with KDE")
ax[3].set_xscale("log")

plt.suptitle("Color Intensity Distributions")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

stats.ttest_ind(man_high["price"], man_low["price"], equal_var=False)

# mean and std values from the sample
n = man_poi["price"].count()
mean = man_poi["price"].mean()
std = man_poi["price"].std()/ np.sqrt(n)

# sample mean
sample_mean = man_high["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

# mean and std values from the sample
n = man_poi["price"].count()
mean = man_poi["price"].mean()
std = man_poi["price"].std()/ np.sqrt(n)

# sample mean
sample_mean = man_low["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

"""* Since we get a significant p value, we can state that there is a significant difference between the mean price of the Airbnb houses that has at least 1 POI and the mean price of the Airbnb houses that do not have any POI which are located in Manhattan.
* Furthermore, by comparing the z-scores, we can indicate that having no POI has more affect on the price than having at least 1 POI, since the mean POI count is not very low in Manhattan.

###Brooklyn

* Starting with detecting outliers and removing them, to get a valid result.
"""

Q1 = bro_poi["price"].quantile(0.25)
Q3 = bro_poi["price"].quantile(0.75)
IQR = Q3 - Q1
print(IQR)

bro_poi.shape

new_bro = bro_poi[~((bro_poi["price"] < (Q1 - 1.5 * IQR)) |(bro_poi["price"] > (Q3 + 1.5 * IQR)))]
new_bro.shape

bro_high = new_bro[new_bro["POI"] >= 1]
bro_low = new_bro[new_bro["POI"] < 1]

bro_poi.describe()

fig, ax = plt.subplots(1, 4, figsize=(14,5))

bro_high['price'].plot(kind="hist", ax=ax[0], bins=10, label="more than 1 poi", color="c", density=True)
ax[0].set_title("Target: 0")
ax[0].set_xscale("log")

bro_poi['price'].plot(kind="hist", ax=ax[1], bins=10, label="random sample", color="m", density=True)
ax[1].set_title("Target: 1")
ax[1].set_xscale("log")

bro_low['price'].plot(kind="hist", ax=ax[2], bins=10, label="less than 1 poi", color="y", density=True)
ax[2].set_title("Target: 2")
ax[2].set_xscale("log")

sns.kdeplot(bro_high['price'], shade=True, label="Target: 0", ax=ax[3], color="c")
sns.kdeplot(bro_poi['price'], shade=True, label="Target: 1", ax=ax[3], color="m")
sns.kdeplot(bro_low['price'], shade=True, label="Target: 2", ax=ax[3], color="y")
ax[3].set_title("Comparison with KDE")
ax[3].set_xscale("log")

plt.suptitle("Color Intensity Distributions")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

stats.ttest_ind(bro_high["price"], bro_low["price"], equal_var=False) #high and low are sig. different

# mean and std values from the sample
n = bro_poi["price"].count()
mean = bro_poi["price"].mean()
std = bro_poi["price"].std()/ np.sqrt(n)

# sample mean
sample_mean = bro_high["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

# mean and std values from the sample
n = bro_poi["price"].count()
mean = bro_poi["price"].mean()
std = bro_poi["price"].std()/ np.sqrt(n)

# sample mean
sample_mean = bro_low["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

"""* Since we get a significant p value, we can state that there is a significant difference between the mean price of the Airbnb houses that has at least 1 POI and the mean price of the Airbnb houses that do not have any POI which are located in Brooklyn.
* Furthermore, by comparing the z-scores, we can indicate that having at least 1 POI has more affect on the price than having no POI, since the mean POI count is not very high in Brooklyn.

###Staten Island

* Starting with detecting outliers and removing them, to get a valid result.
"""

Q1 = stat_poi["price"].quantile(0.25)
Q3 = stat_poi["price"].quantile(0.75)
IQR = Q3 - Q1
print(IQR)

stat_poi.shape

new_stat = stat_poi[~((stat_poi["price"] < (Q1 - 1.5 * IQR)) |(stat_poi["price"] > (Q3 + 1.5 * IQR)))]
new_stat.shape

stat_high = new_stat[new_stat["POI"] >= 1]
stat_low = new_stat[new_stat["POI"] < 1]

fig, ax = plt.subplots(1, 4, figsize=(14,5))

stat_high['price'].plot(kind="hist", ax=ax[0], bins=10, label="more than 1 poi", color="c", density=True)
ax[0].set_title("Target: 0")
ax[0].set_xscale("log")

stat_poi['price'].plot(kind="hist", ax=ax[1], bins=10, label="random sample", color="m", density=True)
ax[1].set_title("Target: 1")
ax[1].set_xscale("log")

stat_low['price'].plot(kind="hist", ax=ax[2], bins=10, label="less than 1 poi", color="y", density=True)
ax[2].set_title("Target: 2")
ax[2].set_xscale("log")

sns.kdeplot(stat_high['price'], shade=True, label="Target: 0", ax=ax[3], color="c")
sns.kdeplot(stat_poi['price'], shade=True, label="Target: 1", ax=ax[3], color="m")
sns.kdeplot(stat_low['price'], shade=True, label="Target: 2", ax=ax[3], color="y")
ax[3].set_title("Comparison with KDE")
ax[3].set_xscale("log")

plt.suptitle("Color Intensity Distributions")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

stats.ttest_ind(stat_high["price"], stat_low["price"], equal_var=False)

# mean and std values from the sample
n = stat_poi["price"].count()
mean = stat_poi["price"].mean()
std = stat_poi["price"].std()/ np.sqrt(n)

# sample mean (drugged rats)
sample_mean = stat_high["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

# mean and std values from the sample
n = stat_poi["price"].count()
mean = stat_poi["price"].mean()
std = stat_poi["price"].std()/ np.sqrt(n)

# sample mean (drugged rats)
sample_mean = stat_low["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

"""* Since the p value is greater than 0.05, we can not reject the null hypothesis, and state that the mean price of the houses that has at least 1 POI is the same as the mean price of the houses that do not have any POI in Staten Island.
* Also by analyzing the z-scores, we can state that the mean price of the houses that has at least 1 POI around are lower than the population mean.

###Bronx

* Starting with detecting outliers and removing them, to get a valid result.
"""

Q1 = bronx_poi["price"].quantile(0.25)
Q3 = bronx_poi["price"].quantile(0.75)
IQR = Q3 - Q1
print(IQR)

bronx_poi.shape

new_bronx = bronx_poi[~((bronx_poi["price"] < (Q1 - 1.5 * IQR)) |(bronx_poi["price"] > (Q3 + 1.5 * IQR)))]
new_bronx.shape

bronx_high = new_bronx[new_bronx["POI"] >= 1] #GROUP 1
bronx_low = new_bronx[new_bronx["POI"] < 1] #GROUP 2

bronx_poi["POI"].describe()

fig, ax = plt.subplots(1, 4, figsize=(14,5))

bronx_high['price'].plot(kind="hist", ax=ax[0], bins=10, label="more than 1 poi", color="c", density=True)
ax[0].set_title("Target: 0")
ax[0].set_xscale("log")

bronx_poi['price'].plot(kind="hist", ax=ax[1], bins=10, label="random sample", color="m", density=True)
ax[1].set_title("Target: 1")
ax[1].set_xscale("log")

bronx_low['price'].plot(kind="hist", ax=ax[2], bins=10, label="less than 1 poi", color="y", density=True)
ax[2].set_title("Target: 2")
ax[2].set_xscale("log")

sns.kdeplot(bronx_high['price'], shade=True, label="Target: 0", ax=ax[3], color="c")
sns.kdeplot(bronx_poi['price'], shade=True, label="Target: 1", ax=ax[3], color="m")
sns.kdeplot(bronx_low['price'], shade=True, label="Target: 2", ax=ax[3], color="y")
ax[3].set_title("Comparison with KDE")
ax[3].set_xscale("log")

plt.suptitle("Color Intensity Distributions")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

stats.ttest_ind(bronx_high["price"], bronx_low["price"], equal_var=False)

# mean and std values from the sample
n = bronx_poi["price"].count()
mean = bronx_poi["price"].mean()
std = bronx_poi["price"].std()/ np.sqrt(n)

# sample mean
sample_mean = bronx_high["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

# mean and std values from the sample
n = bronx_poi["price"].count()
mean = bronx_poi["price"].mean()
std = bronx_poi["price"].std()/ np.sqrt(n)

# sample mean
sample_mean = bronx_low["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

"""* Since the p value is greater than 0.05, we can not reject the null hypothesis, and state that the mean price of the houses that has at least 1 POI is the same as the mean price of the houses that do not have any POI in Bronx.
* Also by analyzing the z-scores, we can state that the mean price of the houses that has at least 1 POI around is higher than the population mean; the mean price of the houses that has no POI is less than the population mean.

###Queens

* Starting with detecting outliers and removing them, to get a valid result.
"""

Q1 = queens_poi["price"].quantile(0.25)
Q3 = queens_poi["price"].quantile(0.75)
IQR = Q3 - Q1
print(IQR)

queens_poi.shape

new_queens = queens_poi[~((queens_poi["price"] < (Q1 - 1.5 * IQR)) |(queens_poi["price"] > (Q3 + 1.5 * IQR)))]
new_queens.shape

queens_high = new_queens[new_queens["POI"] >= 1] #GROUP 1
queens_low = new_queens[new_queens["POI"] < 1] #GROUP 2

"""* Now, since we divided the dataset into 2 groups, let's visualize the differences of these 2 groups' means and the population mean."""

fig, ax = plt.subplots(1, 4, figsize=(14,5))

queens_high['price'].plot(kind="hist", ax=ax[0], bins=10, label="more than 1 poi", color="c", density=True)
ax[0].set_title("Target: 0")
ax[0].set_xscale("log")

new_queens['price'].plot(kind="hist", ax=ax[1], bins=10, label="random sample", color="m", density=True)
ax[1].set_title("Target: 1")
ax[1].set_xscale("log")

queens_low['price'].plot(kind="hist", ax=ax[2], bins=10, label="less than 1 poi", color="y", density=True)
ax[2].set_title("Target: 2")
ax[2].set_xscale("log")

sns.kdeplot(bronx_high['price'], shade=True, label="HIGH POI", ax=ax[3], color="c")
sns.kdeplot(bronx_poi['price'], shade=True, label="POPULATION", ax=ax[3], color="m")
sns.kdeplot(bronx_low['price'], shade=True, label="LOW POI", ax=ax[3], color="y")
ax[3].set_title("Comparison with KDE")
ax[3].set_xscale("log")

plt.suptitle("Color Intensity Distributions")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

stats.ttest_ind(queens_high["price"], queens_low["price"], equal_var=False)

# mean and std values from the sample
n = new_queens["price"].count()
mean = new_queens["price"].mean()
std = new_queens["price"].std()/ np.sqrt(n)

# sample mean
sample_mean = queens_high["price"].mean() #Z-score of high POI

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

# mean and std values from the sample
n = new_queens["price"].count()
mean = new_queens["price"].mean()
std = new_queens["price"].std()/ np.sqrt(n)

# sample mean
sample_mean = queens_low["price"].mean() #Z-score of low POI

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

"""* By the obtained p value, we can say that there is a significant difference between the mean price of the houses that has at least 1 POI in 0.8 km, and the mean price of the houses that do not have any.
* Also by looking at their z-scores, we can state that having at least 1 POI has more affect on the price than having no POI.
"""



"""##The Effects of Proximity of POIs to Airbnb Houses

* In this section, we analyzed the relationship between POI counts and the features of Airbnb houses by the obtained p values from linear regression. Also visualized corresponding linear relationships in every subsection.
"""

main_df["POI"] = main_df.apply(lambda x: find_poi_count(x["latitude"],x["longitude"]), axis = 1)
main_df.head()

main_df.dropna(inplace=True)

"""Here, we performed one hot encoding to analyze the relationship between POI counts and categorical variables."""

roomtype_coded = pd.get_dummies(main_df["room_type"])
roomtype_coded.head()

neighbourhood_coded = pd.get_dummies(main_df["neighbourhood_group"])
neighbourhood_coded.head()

"""###Room Type

"""

from scipy.stats import linregress

slope, intercept, r, p, se = linregress(roomtype_coded["Entire home/apt"], main_df["POI"])
print(p)
print(slope)

slope, intercept, r, p, se = linregress(roomtype_coded["Private room"], main_df["POI"])
print(p)
print(slope)

slope, intercept, r, p, se = linregress(roomtype_coded["Shared room"], main_df["POI"])
print(p)
print(slope)

"""* Using these p values, we can conclude that as POI counts increase, all types of houses increase in a linear fashion. However, as we can observe from the slope values we obtained, there is a positive linear relationship between Entire home and POI counts, where on the other hand there is a negative linear relationship between Private/Shared rooms and POI counts.
* As a conclusion, we can say that as the POI count increase, Entire home/apt counts also increase; Private room counts decrease faster than Shared room counts.

###Minimum Nights
"""

slope, intercept, r, p, se = linregress(main_df["minimum_nights"], main_df["POI"])
print(p)
print(slope)

"""* From this p result, it is possible to say that there is a
positive linear relationship between the Point of Interest counts and minimum nights.
"""

from scipy.stats import linregress
y = main_df["POI"]
x = main_df["minimum_nights"]
res = linregress(x, y)
plt.plot(x, y, 'o', label='minimum nights')
plt.plot(x, res.intercept + res.slope*x, 'r', label='fitted line')
plt.legend()
plt.show()

"""* From this p result, it is possible to say that there is a
positive linear relationship between the Point of Interest counts and minimum nights.

###Price
"""

slope, intercept, r, p, se = linregress(main_df["price"], main_df["POI"])
print(p)
print(slope)

y = main_df["POI"]
x = main_df["price"]
res = linregress(x, y)
plt.plot(x, y, 'o', label='price')
plt.plot(x, res.intercept + res.slope*x, 'r', label='fitted line')
plt.legend()
plt.show()

"""* As we also analyzed in the previous section that there is a significant difference between the houses that has at least 1 POI in 0.8 km are more expensive than the ones that don't have any POI; we can also conclude from this linear regression analysis that there is a positive linear relationship between POI counts and price.

###Number of Reviews and Reviews per Month
"""

y = main_df["POI"]
x = main_df["number_of_reviews"]
res = linregress(x, y)
plt.plot(x, y, 'o', label='# of reviews')
plt.plot(x, res.intercept + res.slope*x, 'r', label='fitted line')
plt.legend()
plt.show()

slope, intercept, r, p, se = linregress(main_df["number_of_reviews"], main_df["POI"])
print(p)
print(slope)

y = main_df["POI"]
x = main_df["reviews_per_month"]
res = linregress(x, y)
plt.plot(x, y, 'o', label='reviews per month')
plt.plot(x, res.intercept + res.slope*x, 'r', label='fitted line')
plt.legend()
plt.show()

slope, intercept, r, p, se = linregress(main_df["reviews_per_month"], main_df["POI"])
print(p)
print(slope)

"""* By looking at these p values, we can indicate that there is a negative linear relationship between both Number of Reviews and Reviews per Month, and POI counts.

###Calculated Host Listings Count
"""

slope, intercept, r, p, se = linregress(main_df["calculated_host_listings_count"], main_df["POI"])
print(p)
print(slope)

"""* As we get the p value equals to 0, we can say that there is no significant linear relationship between Calculated Host Listings Counts and POI counts.

###Availability
"""

slope, intercept, r, p, se = linregress(main_df["availability_365"], main_df["POI"])
print(p)
print(slope)

"""* According to the p value we obtained, we can conclude that there is a positive linear relationship between availability and POI counts.

* Since the p value is greater than 0.05, we can say that there is no significant linear relationship between crime counts and POI counts.

##Bus Stop Locations##
In this section, we analyzed our the Bus Stop's statistics and how they vary according to the neighbourhood groups, including their distributions and visual explanations.
"""

from google.colab import drive
drive.mount("./drive", force_remount=True)

path_prefix = "./drive/My Drive"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from os.path import join
from scipy.stats import f_oneway

# %matplotlib inline

filename_main = "AB_NYC_2019.csv"
main_df = pd.read_csv(join(path_prefix, filename_main))
main_df.head()

# This part shows the Bus Stop locations and we do not needed to filter to data because our dataset just include the New York State's Bus Stop Locations
# BoroName shows us to neighborhood and Boro Code is the code of neighborhood group
#such as  ;
# Manhattan --> 1 ,Bronx --> 2 ,Brooklyn --> 3 ,Queens --> 4 ,Staten Island --> 5
file_busstop = "Bus_Stop_Shelter.csv"
busstop_df = pd.read_csv(join(path_prefix, file_busstop))
busstop_df.head()

# We found each BoroName has how mant busstop location and Queens has the most bustop locations and Staten Island has least
neigh_group = busstop_df.groupby(by="BoroName")
neigh_group["BoroCode"].value_counts()

main_busstop_df = pd.DataFrame(main_df, columns = ["name", "neighbourhood_group", "latitude", "longitude", "price"])
main_busstop_df.head(10)

# We add new coloumn as Bus Stop to our main dataset
main_busstop_df["Bus Stop"] = 0.0
main_busstop_df.head(10)

import math
from sklearn.neighbors import BallTree
from math import radians

house_locs = main_busstop_df[["latitude", "longitude"]].values
busstop_locs = busstop_df[["LATITUDE", "LONGITUDE"]].values

# a function for radians
r = np.vectorize(lambda x: radians(x))
# lat-lng change values to radian

house_locs = r(house_locs)
busstop_locs = r(busstop_locs)

# haversine, is points for distances
tree = BallTree(busstop_locs, metric="haversine")
nearest_distances, nearest_busstop_indices = tree.query(house_locs)

for i in range(len(house_locs)-1) :
  main_busstop_df["Bus Stop"][i] = nearest_distances[i]

print(nearest_distances[0])

main_busstop_df.head()
# We calculated the distance between the busstop and airBnB houses

main_busstop_df["Bus Stop"] = (main_busstop_df["Bus Stop"] - main_busstop_df["Bus Stop"].min()) / (main_busstop_df["Bus Stop"].max() - main_busstop_df["Bus Stop"].min())
main_busstop_df["Bus Stop"].describe()
# This is description of Busstop distance between houses with meand 0.1229 and standart deviation of 0.0851, max is.

main_busstop_df["price"].describe()
# Desciription of prices with mean 152 and standart deviation of 240, max is 10000

# We found treshhold from the meand of distance between houses and bus stop locations.
treshold = 0.122960
high_busstop_df = main_busstop_df.loc[main_busstop_df["Bus Stop"] >= treshold]
high_busstop_df.head()

# We add a new column to our maindataset as Bus Stop and it shows thedistance between to house and bus stop, we gropuded our data if it has the lower distance than the mean.
low_busstop_df = main_busstop_df.loc[main_busstop_df["Bus Stop"] < treshold]
low_busstop_df.head()

from scipy import stats
from scipy import special
from os.path import join
import seaborn as sns
# mean and std values from the sample
n =  main_busstop_df["price"].count()
mean = main_busstop_df["price"].mean()
std = main_busstop_df["price"].std() / np.sqrt(n)
offset = 4*std

# the x-axis ticks of the plot
# generates 100 equally separated ticks
x = np.linspace(mean - offset, mean + offset, n)

# probability density function
# of the given normal dist.
y = stats.norm.pdf(x, mean, std)

plt.figure(figsize=(10, 5))
plt.plot(x,y)
# put grids on the figure
plt.grid()
plt.xlabel("x")
plt.ylabel("Normal Distribution")
plt.fill_between(x, y, alpha=0.3, color="y")
plt.title(f"PRICE DISTRIBUTION")
plt.show()
# Normal distribition of price distribution.

fig, ax = plt.subplots(1, 4, figsize=(14,5))

high_busstop_df['price'].plot(kind="hist", ax=ax[0], bins=100, label="more than 1 bus stop", color="c", density=True)
ax[0].set_title("Target: 0")
ax[0].set_xscale("log")

main_busstop_df['price'].plot(kind="hist", ax=ax[1], bins=100, label="random sample", color="m", density=True)
ax[1].set_title("Target: 1")
ax[1].set_xscale("log")

low_busstop_df['price'].plot(kind="hist", ax=ax[2], bins=100, label="less than 1 bus stop", color="y", density=True)
ax[2].set_title("Target: 2")
ax[2].set_xscale("log")

sns.kdeplot(high_busstop_df['price'], shade=True, label="Target: 0", ax=ax[3], color="c")
sns.kdeplot(main_busstop_df['price'], shade=True, label="Target: 1", ax=ax[3], color="m")
sns.kdeplot(low_busstop_df['price'], shade=True, label="Target: 2", ax=ax[3], color="y")
ax[3].set_title("Comparison with KDE")
ax[3].set_xscale("log")

plt.suptitle("Color Intensity Distributions")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# mean and std values from the sample
n =  main_busstop_df["price"].count()
mean = main_busstop_df["price"].mean()
std = main_busstop_df["price"].std() / np.sqrt(n)

# sample mean
sample_mean = high_busstop_df["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

# mean and std values from the sample
n =  main_busstop_df["price"].count()
mean = main_busstop_df["price"].mean()
std = main_busstop_df["price"].std() / np.sqrt(n)

# sample mean
sample_mean = low_busstop_df["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

stats.ttest_ind(high_busstop_df["price"], low_busstop_df["price"], equal_var=False)
# T-test

"""###Statistical tests to check how (or if) features affect Airbnb prices in different neighborhood

"""

bus_man = main_busstop_df[main_busstop_df["neighbourhood_group"] == "Manhattan"]
bus_brook = main_busstop_df[main_busstop_df["neighbourhood_group"] == "Brooklyn"]
bus_stat = main_busstop_df[main_busstop_df["neighbourhood_group"] == "Staten Island"]
bus_queens = main_busstop_df[main_busstop_df["neighbourhood_group"] == "Queens"]
bus_bronx = main_busstop_df[main_busstop_df["neighbourhood_group"] == "Bronx"]
# We categorized our data as their BoroNames

"""####Manhattan"""

# Founded from the bus stop distances to airbnb houses (0.122960), if it has lower or higher distances to house
man_high = bus_man[bus_man["Bus Stop"] >= 0.122960]
man_low = bus_man[bus_man["Bus Stop"] < 0.122960]

# desciription of data
man_bus.describe()



stats.ttest_ind(man_high["price"], man_low["price"], equal_var=False)
# p < 0.05 -> significant difference
# T test

# mean and std values from the sample
n = man_bus["price"].count()
mean = man_bus["price"].mean()
std = man_bus["price"].std()/ np.sqrt(n)

# sample mean
sample_mean = man_high["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

"""####Brooklyn"""

# Founded from the bus stop distances to airbnb houses (0.122960), if it has lower or higher distances to house
brook_high = bus_brook[bus_brook["Bus Stop"] >= 0.122960]
brook_low = bus_brook[bus_brook["Bus Stop"] < 0.122960]

#Desciription of our data
brook_bus.describe()

fig, ax = plt.subplots(1, 4, figsize=(14,5))

brook_high['price'].plot(kind="hist", ax=ax[0], bins=100, label="more than 1 Bus Stop", color="c", density=True)
ax[0].set_title("Target: 0")
ax[0].set_xscale("log")

brook_bus['price'].plot(kind="hist", ax=ax[1], bins=100, label="random sample", color="m", density=True)
ax[1].set_title("Target: 1")
ax[1].set_xscale("log")

brook_low['price'].plot(kind="hist", ax=ax[2], bins=100, label="less than 1 Bus Stop", color="y", density=True)
ax[2].set_title("Target: 2")
ax[2].set_xscale("log")

sns.kdeplot(brook_high['price'], shade=True, label="Target: 0", ax=ax[3], color="c")
sns.kdeplot(brook_bus['price'], shade=True, label="Target: 1", ax=ax[3], color="m")
sns.kdeplot(brook_low['price'], shade=True, label="Target: 2", ax=ax[3], color="y")
ax[3].set_title("Comparison with KDE")
ax[3].set_xscale("log")

plt.suptitle("Color Intensity Distributions")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

stats.ttest_ind(brook_high["price"], brook_low["price"], equal_var=False)
# p < 0.05 -> significant difference

# mean and std values from the sample
n = brook_bus["price"].count()
mean = brook_bus["price"].mean()
std = brook_bus["price"].std()/ np.sqrt(n)

# sample mean
sample_mean = brook_high["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

"""####Staten Island"""

# Founded from the bus stop distances to airbnb houses (0.122960), if it has lower or higher distances to house
stat_high = bus_stat[bus_stat["Bus Stop"] >= 0.122960]
stat_low = bus_stat[bus_stat["Bus Stop"] < 0.122960]

#Desciription of our data
brook_bus.describe()

fig, ax = plt.subplots(1, 4, figsize=(14,5))

stat_high['price'].plot(kind="hist", ax=ax[0], bins=100, label="more than 1 Bus Stop", color="c", density=True)
ax[0].set_title("Target: 0")
ax[0].set_xscale("log")

stat_bus['price'].plot(kind="hist", ax=ax[1], bins=100, label="random sample", color="m", density=True)
ax[1].set_title("Target: 1")
ax[1].set_xscale("log")

stat_low['price'].plot(kind="hist", ax=ax[2], bins=100, label="less than 1 Bus Stop", color="y", density=True)
ax[2].set_title("Target: 2")
ax[2].set_xscale("log")

sns.kdeplot(stat_high['price'], shade=True, label="Target: 0", ax=ax[3], color="c")
sns.kdeplot(stat_bus['price'], shade=True, label="Target: 1", ax=ax[3], color="m")
sns.kdeplot(stat_low['price'], shade=True, label="Target: 2", ax=ax[3], color="y")
ax[3].set_title("Comparison with KDE")
ax[3].set_xscale("log")

plt.suptitle("Color Intensity Distributions")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

stats.ttest_ind(stat_high["price"], stat_low["price"], equal_var=False)
# p < 0.05 -> significant difference
# T-test

# mean and std values from the sample
n = stat_bus["price"].count()
mean = stat_bus["price"].mean()
std = stat_bus["price"].std()/ np.sqrt(n)

# sample mean
sample_mean = stat_high["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

"""####Queens"""

# Founded from the bus stop distances to airbnb houses (0.122960), if it has lower or higher distances to house
queens_high = bus_queens[bus_queens["Bus Stop"] >= 0.122960]
queens_low = bus_queens[bus_queens["Bus Stop"] < 0.122960]

#Desciription of our data
queens_bus.describe()

fig, ax = plt.subplots(1, 4, figsize=(14,5))

queens_high['price'].plot(kind="hist", ax=ax[0], bins=100, label="more than 1 Bus Stop", color="c", density=True)
ax[0].set_title("Target: 0")
ax[0].set_xscale("log")

queens_bus['price'].plot(kind="hist", ax=ax[1], bins=100, label="random sample", color="m", density=True)
ax[1].set_title("Target: 1")
ax[1].set_xscale("log")

queens_low['price'].plot(kind="hist", ax=ax[2], bins=100, label="less than 1 Bus Stop", color="y", density=True)
ax[2].set_title("Target: 2")
ax[2].set_xscale("log")

sns.kdeplot(queens_high['price'], shade=True, label="Target: 0", ax=ax[3], color="c")
sns.kdeplot(queens_bus['price'], shade=True, label="Target: 1", ax=ax[3], color="m")
sns.kdeplot(queens_low['price'], shade=True, label="Target: 2", ax=ax[3], color="y")
ax[3].set_title("Comparison with KDE")
ax[3].set_xscale("log")

plt.suptitle("Color Intensity Distributions")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

stats.ttest_ind(queens_high["price"], queens_low["price"], equal_var=False)
# p < 0.05 -> significant difference
#T-test

# mean and std values from the sample
n = queens_bus["price"].count()
mean = queens_bus["price"].mean()
std = queens_bus["price"].std()/ np.sqrt(n)

# sample mean
sample_mean = queens_high["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

"""####Bronx"""

# Founded from the bus stop distances to airbnb houses (0.122960), if it has lower or higher distances to house
bronx_high = bus_bronx[bus_bronx["Bus Stop"] >= 0.122960]
bronx_low = bus_bronx[bus_bronx["Bus Stop"] < 0.122960]

#Desciription of our data
bronx_bus.describe()

fig, ax = plt.subplots(1, 4, figsize=(14,5))

bronx_high['price'].plot(kind="hist", ax=ax[0], bins=100, label="more than 1 Bus Stop", color="c", density=True)
ax[0].set_title("Target: 0")
ax[0].set_xscale("log")

bronx_bus['price'].plot(kind="hist", ax=ax[1], bins=100, label="random sample", color="m", density=True)
ax[1].set_title("Target: 1")
ax[1].set_xscale("log")

bronx_low['price'].plot(kind="hist", ax=ax[2], bins=100, label="less than 1 Bus Stop", color="y", density=True)
ax[2].set_title("Target: 2")
ax[2].set_xscale("log")

sns.kdeplot(queens_high['price'], shade=True, label="Target: 0", ax=ax[3], color="c")
sns.kdeplot(queens_bus['price'], shade=True, label="Target: 1", ax=ax[3], color="m")
sns.kdeplot(queens_low['price'], shade=True, label="Target: 2", ax=ax[3], color="y")
ax[3].set_title("Comparison with KDE")
ax[3].set_xscale("log")

plt.suptitle("Color Intensity Distributions")
# To avoid suptitle and titles of ax titles colliding
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

stats.ttest_ind(bronx_high["price"], queens_low["price"], equal_var=False)
# p < 0.05 -> significant difference
#T test

# mean and std values from the sample
n = bronx_bus["price"].count()
mean = bronx_bus["price"].mean()
std = bronx_bus["price"].std()/ np.sqrt(n)

# sample mean
sample_mean = bronx_high["price"].mean()

# calculating the z-score
z_score = (sample_mean - mean) / std

print("z-score: {}".format(z_score))

"""## NYPD Crime Statistics#
In this section, we analyze the NYPD crime database and how they vary with each neigborhood group, including their distributions and visual explanations.

At the end of this section, we will have a crime score for each neigborhood group
(we will score felony = 3, misdemeanor = 2, violation = 1)

source of scoring for each offense type: https://patrickparrottalaw.com/differences-between-a-violation-a-misdemeanor-and-a-felony/
"""

import pandas as pd
from os.path import join
nypd_filename = "NYPD_Complaint_Data_Current__Year_To_Date_.csv"
nypd_df = pd.read_csv(join(path_prefix, nypd_filename))

nypd_df.describe()

nypd_df.info()

#nypd_df.loc[nypd_df["neighbourhood_group"]]
#used for testing purposes

#I now have the neighborhood_group of each criminal offense, now I want to create a new dataset that only contains each neighborhood and
#how many of each crime is reported there
import csv

if __name__ == '__main__':
    with open('neighborhoods.csv', 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        for row in (('neighborhood', 'Felony', 'Misdemeanor', 'Violation', 'Score'), ('Queens', 0, 0, 0, 0.0),
                    ('Manhattan', 0, 0, 0, 0.0), ('Brooklyn', 0, 0, 0, 0.0), ('Bronx', 0, 0, 0, 0.0), ('Staten Island', 0, 0, 0, 0.0)):
            writer.writerow(row)

#opens a clean indexation board, it is filled a few cells down
neighborhoods_filename = "neighborhoods.csv"
nh_df = pd.read_csv(neighborhoods_filename)

#keep in mind that there are ~10700 entries in nypd database that have no BORO_NM value,
#take care of those when the lat-lon function is fixed

#alternate idea: ignore them? 10k is very small compared to 400k after all

for i in range(413412):
  if nypd_df["BORO_NM"][i] == "QUEENS":
    if nypd_df["LAW_CAT_CD"][i] == "FELONY":
      nh_df["Felony"][0] = nh_df["Felony"][0]+1
    elif nypd_df["LAW_CAT_CD"][i] == "MISDEMEANOR":
      nh_df["Misdemeanor"][0] = nh_df["Misdemeanor"][0]+1
    else: #crime is "violation"
      nh_df["Violation"][0] = nh_df["Violation"][0]+1

  elif nypd_df["BORO_NM"][i] == "MANHATTAN":
    if nypd_df["LAW_CAT_CD"][i] == "FELONY":
      nh_df["Felony"][1] = nh_df["Felony"][1]+1
    elif nypd_df["LAW_CAT_CD"][i] == "MISDEMEANOR":
      nh_df["Misdemeanor"][1] = nh_df["Misdemeanor"][1]+1
    else: #crime is "violation"
      nh_df["Violation"][1] = nh_df["Violation"][1]+1

  elif nypd_df["BORO_NM"][i] == "BROOKLYN":
    if nypd_df["LAW_CAT_CD"][i] == "FELONY":
      nh_df["Felony"][2] = nh_df["Felony"][2]+1
    elif nypd_df["LAW_CAT_CD"][i] == "MISDEMEANOR":
      nh_df["Misdemeanor"][2] = nh_df["Misdemeanor"][2]+1
    else: #crime is "violation"
      nh_df["Violation"][2] = nh_df["Violation"][2]+1

  elif nypd_df["BORO_NM"][i] == "BRONX":
    if nypd_df["LAW_CAT_CD"][i] == "FELONY":
      nh_df["Felony"][3] = nh_df["Felony"][3]+1
    elif nypd_df["LAW_CAT_CD"][i] == "MISDEMEANOR":
      nh_df["Misdemeanor"][3] = nh_df["Misdemeanor"][3]+1
    else: #crime is "violation"
      nh_df["Violation"][3] = nh_df["Violation"][3]+1

  elif nypd_df["BORO_NM"][i] == "STATEN ISLAND":
    if nypd_df["LAW_CAT_CD"][i] == "FELONY":
      nh_df["Felony"][3] = nh_df["Felony"][3]+1
    elif nypd_df["LAW_CAT_CD"][i] == "MISDEMEANOR":
      nh_df["Misdemeanor"][3] = nh_df["Misdemeanor"][3]+1
    else: #crime is "violation"
      nh_df["Violation"][3] = nh_df["Violation"][3]+1

  else:# nypd_df["BORO_NM"][i] == NULL (none)
    if nypd_df["LAW_CAT_CD"][i] == "FELONY":
      nh_df["Felony"][4] = nh_df["Felony"][4]+1
    elif nypd_df["LAW_CAT_CD"][i] == "MISDEMEANOR":
      nh_df["Misdemeanor"][4] = nh_df["Misdemeanor"][4]+1
    else: #crime is "violation"
      nh_df["Violation"][4] = nh_df["Violation"][4]+1

#holy crap dude what? Staten Island has virtually no reports whatsoever, that's wild!

nh_df["Score"][0] = (nh_df["Felony"][0]*3) + (nh_df["Misdemeanor"][0]*2) + (nh_df["Violation"][0])
nh_df["Score"][1] = (nh_df["Felony"][1]*3) + (nh_df["Misdemeanor"][1]*2) + (nh_df["Violation"][1])
nh_df["Score"][2] = (nh_df["Felony"][2]*3) + (nh_df["Misdemeanor"][2]*2) + (nh_df["Violation"][2])
nh_df["Score"][3] = (nh_df["Felony"][3]*3) + (nh_df["Misdemeanor"][3]*2) + (nh_df["Violation"][3])
nh_df["Score"][4] = (nh_df["Felony"][4]*3) + (nh_df["Misdemeanor"][4]*2) + (nh_df["Violation"][4])

#sums up the total score of the area we're looking into and gets the % of each area, writing it as the score of that area
score_sum = 0
for i in range(5):
  score_sum = score_sum + nh_df["Score"][i]

for i in range(5):
  nh_df["Score"][i] = (nh_df["Score"][i]/score_sum)*100
nh_df.head()

"""###Visualizations of the features


---



---
"""

#implement pandas graphs here.
#1- get a bar chart that displays each area and its crime stats
#2- get a map that gives a heatmap of the crimes
import folium
!pip install folium.plugins
from folium.plugins import HeatMap

crime_map = folium.Map(location=[40.786764, -73.935811], zoom_start=10, width=1300, height=700, tiles = "Stamen Terrain")

locations = nypd_df[["Latitude", "Longitude"]].dropna(how="any").values

HeatMap(locations, radius = 11).add_to(crime_map)
# radius is set to 11 so that the map becomes more legible

crime_map

#credits: code modified from https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html

import matplotlib
import matplotlib.pyplot as plt
import numpy as np

labels = nh_df["neighborhood"]
Felonies = nh_df["Felony"]
Misdem = nh_df["Misdemeanor"]
Viol = nh_df["Violation"]

x = np.arange(len(labels))  # the label locations
width = 0.25  # the width of the bars

crime_fig, ax = plt.subplots()
rects1 = ax.bar(x - width, Felonies, width, label='Felony')
rects2 = ax.bar(x, Misdem, width, label='Misdem.')
rects3 = ax.bar(x + width, Viol, width, label='Violation')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Scores')
ax.set_title('# of crimes per region')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()


def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()-1000 #-1000 because the misdemeanor reports label clips into the title otherwise
        ax.annotate('{}'.format(height),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')


autolabel(rects1)
autolabel(rects2)
autolabel(rects3)

crime_fig.tight_layout()

plt.show()

"""###Statistical Similarities and Differences

Our analysis of the NYPD crime datasheet shows that Brooklyn has the highest crime score, while Staten Island has the lowest crime score among the locations being investigated. If our crime scoring is to be taken as the basis, each region in NY can be listed as follows (listed from highest to lowest crime score): Brooklyn, Bronx, Manhattan, Queens, and Staten Island

In the heatmap, one might notice that due to the amount of crime, new york is not very visible. But due to the low crime reports in Staten Island, one can actually make out the map after zooming in; something that is not so easily achieved with the other regions

###Statistical Tests to Check How crime Counts Affect Airbnb Prices

In this section we want to test if there are significant differences in terms of proximity to crimes commited in New York, and prices of airBnB locations.

To do this analysis, we analyzed the data in 2 ways: First, we checked the regional averages of crime% and price, and compared their crime/price.

**Null Hypothesis ($H_0$)**: The `crime/average regional price` of airBnB locations does not change among these regions. ($ci_1$ denotes the average crime/price for New York, and $ci_2$ denotes average crime/price for each region)

$ H_0: \mu_{ci_1} = \mu_{ci_2} $

**Alternative Hypothesis ($H_A$)**: The `crime/average regional price` of airBnB locations show significant difference among these regions.

$ H_A:$ Means $\mu_{ci_1}, \mu_{ci_2}$ are not same.

**Significance level**: As most of hypothesis tests assume significance level as `0.05`, we are setting it as `0.05` for our test too.

---

Second, we determined the proximity distance as 1 km, and counted the number of crime occurences within this distance, for each house.

**Null Hypothesis ($H_0$)**: The value of `crime/price` for each house in New York does not change ($ci_1$ denotes the average crime/price in New York, and $ci_2$ denotes the crime/price for each house)

$ H_0: \mu_{ci_1} = \mu_{ci_2} $

**Alternative Hypothesis ($H_A$)**: The `crime/price` of airBnB locations show significant difference in New York.

**Significance level**: As most of hypothesis tests assume significance level as `0.05`, we are setting it as `0.05` for our test too.
"""

#TODO: Analyze the data such that you get "CRIME SCORE-PRICE" dataset
# run this data through a t-test

#idea: sum up prices of each region too. That way you can easily analyze region-by-region


#TODO: Analyze the data such that you get "CRIME PROXIMITY (AMOUNT)-PRICE" dataset
#run this data through a z-test

#idea: create a function to count the close-by crimes that happen to an airbnb place
#PROBLEM: what does close-by mean? 5 meters? 20? 50? Decide on this and set the function up
#         after discussing the issue with a few people, it was agreed that a reasonable distance of "close enough" is 250 meters
#         this covers a fairly large area
#PROBLEM 2: 250 meters is fairly large, should I score the crimes according to their closeness? Or is this unnecessary?
#           seems like an overcomplication of the issue. It was also pointed out that KISS principle is a good application here

#utilising code written before in "main dataset" section
filename_main = "AB_NYC_2019.csv"
main_df = pd.read_csv(join(path_prefix, filename_main))

#same as before, opening a new file in order to isolate the data and make analysis easier
if __name__ == '__main__':
    with open('score_price.csv', 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        for row in (('neighborhood', 'price', 'score', 't-dat'),
                    ('Queens', 0.0, 0.0, 0.0), ('Manhattan', 0.0, 0.0, 0.0), ('Brooklyn', 0.0, 0.0, 0.0), ('Bronx', 0.0, 0.0, 0.0), ('Staten Island', 0.0, 0.0, 0.0)):
            writer.writerow(row)

score_price_filename = "score_price.csv"
sp_df = pd.read_csv(score_price_filename)

group_main = main_df.groupby(by = "neighbourhood_group")
sp_mean = group_main["price"].mean()

#best way it seems, I can't directly point at the row-column, which kinda bums me out
sp_df['price'][3] = sp_mean[0]
sp_df['price'][2] = sp_mean[1]
sp_df['price'][1] = sp_mean[2]
sp_df['price'][0] = sp_mean[3]
sp_df['price'][4] = sp_mean[4]
#got the prices on the board. Now to get the score on here as well

for i in range(5):
  sp_df['score'][i] = nh_df['Score'][i]

for i in range(5):
  sp_df['t-dat'][i] = sp_df['score'][i]/sp_df['price'][i]

from scipy.stats import f_oneway
f_stats, p_values = f_oneway(sp_df['t-dat'].values)
p_values

import scipy as scipy
from scipy import stats

#stats.ttest_ind(a=sp_df["t-dat"], b=sp_df["score"], equal_var=False)
tStat, pValue =  scipy.stats.ttest_1samp(sp_df['t-dat'], 0.1724938, axis=0)
print("P-Value:{0} T-Statistic:{1}".format(pValue,tStat))

"""The Hypothesis here was if there was a difference in the crime/price value of a region, with the null hypothesis being that there is no difference in this relation. As the p-value is well above 0.05, we fail to reject the Null hypothesis.

This indicates that there is a statistically significant relation between average crime and average price of a region, which is what we would expect if crime and price are positively correlated, and thus have a relatively unchanging crime/price average.

Below, we will now start looking further into this relation by inspecting the "amount of crime-price" relation and find out if a regional average correlation still holds for individual houses
"""

main_crime_df = pd.DataFrame(main_df, columns = ["name", "neighbourhood_group", "latitude", "longitude", "price"])
main_crime_df["Close_crimes"] = 0
#created a new column that will store the close crimes

#more optimized method. Sorts the list and counts the closest ones
import math
def find_crime_bnb_count_optimized(lat, lon) :
  count = 0
  R = 6373.0

  lat1 = math.radians(lat)
  lon1= math.radians(lon)

  for i in range(int(crime_locs[nearest_crime_indices[0]].size/2)):
    lat2 = math.radians(nypd_df["Latitude"][(nearest_crime_indices[0])[i]])
    lon2 = math.radians(nypd_df["Longitude"][(nearest_crime_indices[0])[i]])
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    distance = R * c
    if (distance < 1.0) :
      count = count + 1
    else: #as soon as distance exceeds the set amount, return. Saves time
      return count
  return count

from sklearn.neighbors import BallTree
from math import radians

house_locs = main_df[["latitude", "longitude"]].values
crime_locs = nypd_df[["Latitude", "Longitude"]].values

# radians için bir fonksiyon
r = np.vectorize(lambda x: radians(x))
# lat-lng değerlerini radians'a çevir

house_locs = r(house_locs)
crime_locs = r(crime_locs)

# ballTree'ye crime listesini gir
# haversine, yeryüzü üzerindeki noktalar arasındaki mesafeler için
tree = BallTree(crime_locs, metric="haversine")

for i in range(len(house_locs)-1):
  # oluşan tree'den listing loc'ları için en yakın noktaları çek
  nearest_distances, nearest_crime_indices = tree.query(house_locs[i:i+2], k=int(len(crime_locs)/40))
  #/2 because the last half of closest crimes are probably not in range. Optimizes the solution a bit

  main_crime_df["Close_crimes"][i] = find_crime_bnb_count_optimized(main_df["latitude"][i], main_df["longitude"][i])

#max(main_crime_df['Close_crimes']) # here to test the max we found. If it was greater than
main_crime_df["crime/price"] = 0
main_crime_df.head()
#max(main_crime_df["Close_crimes"])

for i in range(len(main_crime_df)):
  if main_crime_df["price"][i] != 0:
    main_crime_df["crime/price"][i] = main_crime_df["Close_crimes"][i]/main_crime_df["price"][i]

n = main_crime_df["crime/price"].count()
mean = main_crime_df["crime/price"].mean()
std = main_crime_df["crime/price"].std()/ np.sqrt(n)
#I have the mean and standard deviation of the data. We can now start checking the z-scores by finding sample means
#I will take the mean of neighborhood groups as the sample mean

import csv

if __name__ == '__main__':
    with open('stats.csv', 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        for row in (('neighborhood', "n", "total", 'crime/price_mean', "z-score"), ('Queens', 0, 0, 0.0, 0.0),
                    ('Manhattan', 0, 0, 0.0, 0.0), ('Brooklyn', 0, 0, 0.0, 0.0), ('Bronx', 0, 0, 0.0, 0.0), ('Staten Island', 0, 0, 0.0, 0.0)):
            writer.writerow(row)

stats_df = pd.read_csv("stats.csv")

for i in range(len(main_crime_df)):
  if main_crime_df["neighbourhood_group"][i] == "Queens":
    stats_df["n"][0] = stats_df["n"][0] + 1
    stats_df["total"][0] = stats_df["total"][0] + main_crime_df["crime/price"][i]
  elif main_crime_df["neighbourhood_group"][i] == "Manhattan":
    stats_df["n"][1] = stats_df["n"][1] + 1
    stats_df["total"][1] = stats_df["total"][1] + main_crime_df["crime/price"][i]
  elif main_crime_df["neighbourhood_group"][i] == "Brooklyn":
    stats_df["n"][2] = stats_df["n"][2] + 1
    stats_df["total"][2] = stats_df["total"][2] + main_crime_df["crime/price"][i]
  elif main_crime_df["neighbourhood_group"][i] == "Bronx":
    stats_df["n"][2] = stats_df["n"][2] + 1
    stats_df["total"][2] = stats_df["total"][2] + main_crime_df["crime/price"][i]
  else:
    stats_df["n"][2] = stats_df["n"][2] + 1
    stats_df["total"][2] = stats_df["total"][2] + main_crime_df["crime/price"][i]

for i in range(5):
  stats_df["crime/price_mean"][i] = stats_df["total"][i]/stats_df["n"][i]

for i in range(5):
  stats_df["z-score"][i] = (stats_df["crime/price_mean"][i] - mean)/std

print(mean)
print(std)

stats_df.head()

"""the p-value for each row is as follows:

0 -> too low to determine, but it shows that the p-value is well below 0.05

1 -> too high to determine but, we know that the p-value is well above 0.99

2 -> the p-value is 0.99180

3 -> there is an undetermined error in our data

4 -> there is an undetermined error in our data

---

These results show that while there is indeed a positive correlation between crime and price of an airBnB location in some places (Queens), this Null hypothesis should be rejected in others (Manhattan, Brooklyn). We must, then, conclude that there is no real positive correlation between crime in the range of airBnB locations, and the price of said airBnB location.

So this shows that while the danger level of a region is positiviely correlated with the average price in a region, the amount of crimes committed in the range of an airBnB location does not show a similar correlation in general. These two conclusions provide us crucial informations about the crime and price relations in for each neighborhood in New York.

This conclusion is an interestion one, because the initial hypothesis we derived from the first analysis seems to be due to a different cause from what we first surmised. While there is a correlation between regional crime score and mean price in each region, this cannot be reduced to each airBnB location where we say that crime in the range of an airBnB location is correlated with its price.

---

This issue raises a new question: what is the actual cause of the first result we achieved in our t-test? Our possible explanation for this is that in large population centers, the airBnB location are closer to centers of the towns and raise their prices. This increases the average price of a region. Additionally, as the population of a region increases, the total crime committed in each region also increases, which gives that region a higher crime score. However, the closeness of airBnB locations to crimes, and their prices are not positively correlated with each other.

##Main Dataset #
In this section, we analyzed our main dataset based on how the price is changing based on neighborhood, categorized the reviews based on room type as not much revieved, normally revieved , mostly revieved. Also found mean of the reviews per month each region and how the price vary with the rewievs with visualization.
"""

# This is our main dataset
filename_main = "AB_NYC_2019.csv"
main_df = pd.read_csv(join(path_prefix, filename_main))
main_df.head()

main_df.describe() # Descriptive statcis of main dataset
# the mean price is: 153
# the mean of minimum night is : 7
# the mean of number of reviews per post is: 23
# the mean of a availability per post in a year is : 112

main_df.dtypes

main_df.info()

group_main = main_df.groupby(by = "neighbourhood_group") # the mean price calculated for the whole dataset was 153,
group_main["price"].mean()                              # now we can compare with respect to neighbourhoods

"""*   We can conclude that the houses in Manhattan increases the mean price of the houses in New York City.

###Visualizations of the features


---



---
"""

g = sns.displot(data=main_df, x="price",kde=True ) # The distribution of prices
g.ax.set_xscale("log")

# PRICES WITH RESPECT TO NEIGHBOURHOOD
import seaborn as sns
sns.barplot(x="neighbourhood_group", y="price", data=main_df)

import seaborn as sns
sns.barplot(x="neighbourhood_group", y="minimum_nights", data=main_df)
# Minimum nights with respect to neighbourhood

# MINIMUM NIGHTS WITH RESPECT TO PRICES
night_mean = group_main["minimum_nights"].mean()
sns.scatterplot(x="price", y="minimum_nights", data=main_df)

group_main["number_of_reviews"].mean()
# Mean of the number of reviews according to neighbourhoods

sns.scatterplot(x="number_of_reviews", y="price", data=main_df)
#Price with respect to number of reviewes

group_main["reviews_per_month"].mean()
# Mean of the review rate according to neighbourhoods

sns.scatterplot(x="reviews_per_month", y="price", data=main_df) #clearly we can see the outliers
#and how to data distributed

sns.barplot(x= "neighbourhood_group",y="calculated_host_listings_count",data = main_df)
# The distribution of calculated_host_listings_count with respect to neighbourhood

# from here it's possible to catch relations
sns.pairplot(main_df[["reviews_per_month", "price", "neighbourhood_group", "number_of_reviews", "minimum_nights", "calculated_host_listings_count"]])

corrmat = main_df.corr()
plt.figure(figsize=(13, 6))
sns.heatmap(corrmat, vmax=1, annot=True, linewidths=.5)
plt.xticks(rotation=30, horizontalalignment="right")
plt.show()
# Heatmap according to data, to visualize and for better understanding

newmain_df = pd.DataFrame(main_df, columns = ["name", "neighbourhood_group", "price","room_type"])
newmain_df.head(10)
# modication of the dataframe by converting it into another dataframe which just include the unecessary informations

# Created a function to find the number of reviews of a post and
# categorized them into three group
def categorize(number_of_reviews):
    if 0 <= number_of_reviews < 20.000000:
        return "not much reviewed"
    elif 20.000000	 <= number_of_reviews < 100.000000:
        return "normally reviewed"
    elif 100.000000 <= number_of_reviews <= 629.000000	:
        return "mostly reviewed"
# We add our new feature as a new coloumn
newmain_df["reviewed"] = main_df["number_of_reviews"].apply(categorize)
newmain_df.head(10)

ax = newmain_df.groupby(by="reviewed") \
  .mean()["price"].plot(kind="barh", color="steelblue")

ax.set_xlabel('price')
ax.set_ylabel(None)
ax.set_title('Mean Price Distribution According to the Review Rates');
# we created a histogram price according to review distribution

!pip install plotly_express
import plotly_express as px
fig = px.scatter_mapbox(main_df, lat="latitude", lon="longitude", hover_name="name", hover_data=["neighbourhood_group","price","room_type"],
                        color_discrete_sequence=["red"], zoom=10, height=500)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()
#open street map of our date included : latitude,longitude, neighbourhood , price and room type

import folium
!pip install folium.plugins
from folium.plugins import HeatMap

map = folium.Map(location=[40.786764, -73.935811], zoom_start=10, width=1000, height=500, tiles = "Stamen Terrain")

locations = main_df[["latitude", "longitude",]].dropna(how="any").values

HeatMap(locations).add_to(map)

map
# visualization of rooms according to their number of occurance with a heat map

"""###Statistical Similarities and Differences
This can be seen that most of rooms are in Manhattan and Brooklyn, also they have the highest mean price per day. On the other hand, this can be said that  Bronx and Queens were the least popular places in NYC. As heat map and open street map shows as most of the rooms are at the center of town center. Also we can see that most of rooms rented in in Manhattan.
"""